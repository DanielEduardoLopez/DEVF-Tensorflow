{"cells":[{"cell_type":"markdown","metadata":{"id":"noGKS3YhsvC9"},"source":["# Series de tiempo multivariable\n","\n","Como vimos en la presentación, los problemas de series de tiempo multivariable se presentan cuando tenemos un dataset con una única columna para representar el tiempo y por cada punto tenemos dos o más valores correspondientes a distintas variables. Un dataset de este tipo debería verse de la siguiente manera:\n","\n","| tiempo | variable1 | variable2 | ... | variableN |\n","|----|----|----|----|----|\n","| 2022-1-1 | 34 | 78 | ... | 112 |\n","| 2022-1-2 | 12 | 79 | ... | 113 |\n","| 2022-1-3 | 55 | 80 | ... | 114 |\n","| 2022-1-4 | 20 | 85 | ... | 115 |\n","| ... | ... | ... | ... | ... |"]},{"cell_type":"markdown","metadata":{"id":"sHwmpbhzsn-_"},"source":["## Graficando nuestra serie de tiempo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fux7mFZfiEKG"},"outputs":[],"source":["import urllib\n","import zipfile\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def plot_series(time, series, format=\"-\", start=0, end=None):\n","    plt.plot(time[start:end], series[start:end], format)\n","    plt.xlabel(\"Time\")\n","    plt.ylabel(\"Value\")\n","    plt.grid(False)\n","\n","def download_and_extract_data():\n","    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n","    urllib.request.urlretrieve(url, 'household_power.zip')\n","    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n","        zip_ref.extractall()\n","\n","download_and_extract_data()\n","\n","def normalize_series(data, min, max):\n","    data = data - min\n","    data = data / max\n","    return data\n","\n","# Reads the dataset from the CSV.\n","df = pd.read_csv('household_power_consumption.csv', sep=',',\n","                  infer_datetime_format=True, index_col='datetime', header=0)\n","\n","# Number of features in the dataset. We use all features as predictors to\n","# predict all features at future time steps.\n","N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n","\n","# Normalizes the data\n","data = df.values\n","data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n","\n","# Splits the data into training and validation sets.\n","SPLIT_TIME = int(len(data) * 0.5) # DO NOT CHANGE THIS\n","print(int(len(data)))\n","time = np.arange(int(len(data)))\n","time_train = time[:SPLIT_TIME]\n","x_train = data[:SPLIT_TIME]\n","time_valid = time[SPLIT_TIME:]\n","x_valid = data[SPLIT_TIME:]\n","\n","plot_series(time, data, start=0, end=500)"]},{"cell_type":"markdown","metadata":{"id":"aaKLlrebvNLW"},"source":["## Instrucciones\n","\n","Las siguientes instrucciones corresponden a un ejercicio de categoría 5 donde se nos indica qué se espera que resolvamos, aquello que ya nos proveen y el puntaje esperado en términos del MAE (error absoluto medio)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-oCK_3TvprK"},"outputs":[],"source":["# ==============================================================================\n","# There are 5 questions in this exam with increasing difficulty from 1-5.\n","# Please note that the weight of the grade for the question is relative to its\n","# difficulty. So your Category 1 question will score significantly less than\n","# your Category 5 question.\n","#\n","# WARNING: Do not use lambda layers in your model, they are not supported\n","# on the grading infrastructure. You do not need them to solve the question.\n","#\n","# WARNING: If you are using the GRU layer, it is advised not to use the\n","# recurrent_dropout argument (you can alternatively set it to 0),\n","# since it has not been implemented in the cuDNN kernel and may\n","# result in much longer training times.\n","#\n","# You must use the Submit and Test button to submit your model\n","# at least once in this category before you finally submit your exam,\n","# otherwise you will score zero for this category.\n","# ==============================================================================\n","#\n","# TIME SERIES QUESTION\n","#\n","# Build and train a neural network to predict time indexed variables of\n","# the multivariate house hold electric power consumption time series dataset.\n","# Using a window of past 24 observations of the 7 variables, the model\n","# should be trained to predict the next 24 observations of the 7 variables.\n","#\n","# ==============================================================================\n","#\n","# ABOUT THE DATASET\n","#\n","# Original Source:\n","# https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption\n","#\n","# The original Individual House Hold Electric Power Consumption Dataset\n","# has Measurements of electric power consumption in one household with\n","# a one-minute sampling rate over a period of almost 4 years.\n","#\n","# Different electrical quantities and some sub-metering values are available.\n","#\n","# For the purpose of the examination we have provided a subset containing\n","# the data for the first 60 days in the dataset. We have also cleaned the\n","# dataset beforehand to remove missing values. The dataset is provided as a\n","# CSV file in the project.\n","#\n","# The dataset has a total of 7 features ordered by time.\n","# ==============================================================================\n","#\n","# INSTRUCTIONS\n","#\n","# Complete the code in following functions:\n","# 1. solution_model()\n","#\n","# Your code will fail to be graded if the following criteria are not met:\n","#\n","# 1. Model input shape must be (BATCH_SIZE, N_PAST = 24, N_FEATURES = 7),\n","#    since the testing infrastructure expects a window of past N_PAST = 24\n","#    observations of the 7 features to predict the next N_FUTURE = 24\n","#    observations of the same features.\n","#\n","# 2. Model output shape must be (BATCH_SIZE, N_FUTURE = 24, N_FEATURES = 7)\n","#\n","# 3. The last layer of your model must be a Dense layer with 7 neurons since\n","#    the model is expected to predict observations of 7 features.\n","#\n","# 4. Don't change the values of the following constants:\n","#    SPLIT_TIME, N_FEATURES, BATCH_SIZE, N_PAST, N_FUTURE, SHIFT, in\n","#    solution_model() (See code for additional note on BATCH_SIZE).\n","#\n","# 5. Code for normalizing the data is provided - don't change it.\n","#    Changing the normalizing code will affect your score.\n","#\n","# 6. Code for converting the dataset into windows is provided - don't change it.\n","#    Changing the windowing code will affect your score.\n","#\n","# 7. Code for setting the seed is provided - don't change it.\n","#\n","# HINT: If you follow all the rules mentioned above and throughout this\n","# question while training your neural network, there is a possibility that a\n","# validation MAE of approximately 0.055 or less on the normalized validation\n","# dataset may fetch you top marks."]},{"cell_type":"markdown","metadata":{"id":"ARAd90g-vxKH"},"source":["## Código que nos provee el examen\n","\n","El siguiente código se provee directamente al iniciar el ejercicio, es muy importante no modificarlo pero sí es necesario revisarlo para entender qué es lo que hace y cómo poder utilizarlo a nuestro favor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5WCWxnzwEEA"},"outputs":[],"source":["import urllib\n","import zipfile\n","\n","import pandas as pd\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# This function downloads and extracts the dataset to the directory that\n","# contains this file.\n","# DO NOT CHANGE THIS CODE\n","# (unless you need to change https to http)\n","def download_and_extract_data():\n","    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n","    urllib.request.urlretrieve(url, 'household_power.zip')\n","    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n","        zip_ref.extractall()\n","\n","\n","# This function normalizes the dataset using min max scaling.\n","# DO NOT CHANGE THIS CODE\n","def normalize_series(data, min, max):\n","    data = data - min\n","    data = data / max\n","    return data\n","\n","\n","# This function is used to map the time series dataset into windows of\n","# features and respective targets, to prepare it for training and\n","# validation. First element of the first window will be the first element of\n","# the dataset. Consecutive windows are constructed by shifting\n","# the starting position of the first window forward, one at a time (indicated\n","# by shift=1). For a window of n_past number of observations of all the time\n","# indexed variables in the dataset, the target for the window\n","# is the next n_future number of observations of these variables, after the\n","# end of the window.\n","\n","# DO NOT CHANGE THIS CODE\n","def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n","    ds = tf.data.Dataset.from_tensor_slices(series)\n","    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n","    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n","    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n","    return ds.batch(batch_size).prefetch(1)\n","\n","# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n","# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n","def mae(y_true, y_pred):\n","    return np.mean(abs(y_true.ravel() - y_pred.ravel()))\n","\n","def model_forecast(model, series, window_size, batch_size):\n","    ds = tf.data.Dataset.from_tensor_slices(series)\n","    ds = ds.window(window_size, shift=1, drop_remainder=True)\n","    ds = ds.flat_map(lambda w: w.batch(window_size))\n","    ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n","    forecast = model.predict(ds)\n","    return forecast"]},{"cell_type":"markdown","metadata":{"id":"J6RbaLBcwSNs"},"source":["## Solution Model: la función que debemos completar\n","\n","En este punto, las instrucciones del examen indican que solution_model es la función que debemos completar. Esencialmente se nos pide que incluyamos código que permita:\n","\n","\n","1.   Leer el conjunto de datos\n","2.   Segmentar el conjunto de datos en el set de entrenamiento y de prueba\n","3. Construir una red neuronal para encontrar un buen valor de learning rate\n","4. Construir una red neuronal que entrene con los datos adecuados, que acepte el valor de learning rate que calculamos y genere el modelo que ocuparemos para realizar el forecast.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":272465,"status":"ok","timestamp":1684199497639,"user":{"displayName":"Andrea Gómez Herrera","userId":"05035961629227231350"},"user_tz":-120},"id":"M7nfYamKA8Gx"},"outputs":[{"name":"stdout","output_type":"stream","text":["86400\n","Inputs shape (batch, time, features): (32, 24, 7)\n","Labels shape (batch, time, features): (32, 24, 7)\n","Epoch 1/20\n","1349/1349 [==============================] - 34s 15ms/step - loss: 0.0176 - mae: 0.1275 - lr: 0.0010\n","Epoch 2/20\n","1349/1349 [==============================] - 19s 14ms/step - loss: 0.0131 - mae: 0.1171 - lr: 0.0011\n","Epoch 3/20\n","1349/1349 [==============================] - 19s 14ms/step - loss: 0.0116 - mae: 0.1063 - lr: 0.0013\n","Epoch 4/20\n","1349/1349 [==============================] - 20s 15ms/step - loss: 0.0105 - mae: 0.0935 - lr: 0.0014\n","Epoch 5/20\n","1349/1349 [==============================] - 18s 14ms/step - loss: 0.0099 - mae: 0.0838 - lr: 0.0016\n","Epoch 6/20\n","1349/1349 [==============================] - 20s 15ms/step - loss: 0.0096 - mae: 0.0780 - lr: 0.0018\n","Epoch 7/20\n","1349/1349 [==============================] - 18s 14ms/step - loss: 0.0094 - mae: 0.0751 - lr: 0.0020\n","Epoch 8/20\n","1349/1349 [==============================] - 18s 14ms/step - loss: 0.0093 - mae: 0.0736 - lr: 0.0022\n","Epoch 9/20\n","1349/1349 [==============================] - 19s 14ms/step - loss: 0.0092 - mae: 0.0727 - lr: 0.0025\n","Epoch 10/20\n","1349/1349 [==============================] - 18s 14ms/step - loss: 0.0091 - mae: 0.0720 - lr: 0.0028\n","Epoch 11/20\n","1349/1349 [==============================] - 19s 14ms/step - loss: 0.0089 - mae: 0.0713 - lr: 0.0032\n","Epoch 12/20\n","1349/1349 [==============================] - 18s 13ms/step - loss: 0.0088 - mae: 0.0705 - lr: 0.0035\n","Epoch 13/20\n","1349/1349 [==============================] - 18s 14ms/step - loss: 0.0087 - mae: 0.0698 - lr: 0.0040\n","Epoch 14/20\n","1349/1349 [==============================] - 18s 13ms/step - loss: 0.0086 - mae: 0.0691 - lr: 0.0045\n","Epoch 15/20\n","1349/1349 [==============================] - 19s 14ms/step - loss: 0.0085 - mae: 0.0683 - lr: 0.0050\n","Epoch 16/20\n","1349/1349 [==============================] - 18s 13ms/step - loss: 0.0084 - mae: 0.0677 - lr: 0.0056\n","Epoch 17/20\n","1349/1349 [==============================] - 19s 14ms/step - loss: 0.0082 - mae: 0.0671 - lr: 0.0063\n","Epoch 18/20\n","1349/1349 [==============================] - 18s 13ms/step - loss: 0.0082 - mae: 0.0666 - lr: 0.0071\n","Epoch 19/20\n","1349/1349 [==============================] - 19s 14ms/step - loss: 0.0081 - mae: 0.0661 - lr: 0.0079\n","Epoch 20/20\n","1349/1349 [==============================] - 19s 14ms/step - loss: 0.0080 - mae: 0.0658 - lr: 0.0089\n"]},{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 640x480 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","1349/1349 [==============================] - 43s 29ms/step - loss: 0.0141 - mae: 0.1082 - val_loss: 0.0096 - val_mae: 0.0766\n","Epoch 2/30\n","1349/1349 [==============================] - 32s 23ms/step - loss: 0.0097 - mae: 0.0786 - val_loss: 0.0091 - val_mae: 0.0738\n","Epoch 3/30\n","1349/1349 [==============================] - 32s 24ms/step - loss: 0.0093 - mae: 0.0763 - val_loss: 0.0089 - val_mae: 0.0728\n","Epoch 4/30\n","1349/1349 [==============================] - 39s 29ms/step - loss: 0.0090 - mae: 0.0746 - val_loss: 0.0087 - val_mae: 0.0715\n","Epoch 5/30\n","1349/1349 [==============================] - 39s 29ms/step - loss: 0.0088 - mae: 0.0730 - val_loss: 0.0084 - val_mae: 0.0696\n","Epoch 6/30\n","1349/1349 [==============================] - 31s 23ms/step - loss: 0.0086 - mae: 0.0714 - val_loss: 0.0082 - val_mae: 0.0677\n","Epoch 7/30\n","1349/1349 [==============================] - 31s 23ms/step - loss: 0.0084 - mae: 0.0699 - val_loss: 0.0080 - val_mae: 0.0663\n","Epoch 8/30\n","1349/1349 [==============================] - 30s 22ms/step - loss: 0.0082 - mae: 0.0688 - val_loss: 0.0079 - val_mae: 0.0651\n","Epoch 9/30\n","1349/1349 [==============================] - 29s 22ms/step - loss: 0.0081 - mae: 0.0678 - val_loss: 0.0078 - val_mae: 0.0640\n","Epoch 10/30\n","1349/1349 [==============================] - 30s 22ms/step - loss: 0.0080 - mae: 0.0669 - val_loss: 0.0077 - val_mae: 0.0631\n","Epoch 11/30\n","1349/1349 [==============================] - 29s 22ms/step - loss: 0.0079 - mae: 0.0660 - val_loss: 0.0076 - val_mae: 0.0625\n","Epoch 12/30\n","1349/1349 [==============================] - 30s 22ms/step - loss: 0.0078 - mae: 0.0653 - val_loss: 0.0076 - val_mae: 0.0619\n","Epoch 13/30\n","1349/1349 [==============================] - 29s 22ms/step - loss: 0.0078 - mae: 0.0646 - val_loss: 0.0075 - val_mae: 0.0615\n","Epoch 14/30\n","1349/1349 [==============================] - 30s 23ms/step - loss: 0.0077 - mae: 0.0640 - val_loss: 0.0074 - val_mae: 0.0611\n","Epoch 15/30\n","1349/1349 [==============================] - 39s 29ms/step - loss: 0.0076 - mae: 0.0634 - val_loss: 0.0074 - val_mae: 0.0608\n","Epoch 16/30\n","1349/1349 [==============================] - 30s 22ms/step - loss: 0.0076 - mae: 0.0629 - val_loss: 0.0073 - val_mae: 0.0606\n","Epoch 17/30\n","1349/1349 [==============================] - 39s 29ms/step - loss: 0.0075 - mae: 0.0624 - val_loss: 0.0073 - val_mae: 0.0604\n","Epoch 18/30\n","1349/1349 [==============================] - 39s 29ms/step - loss: 0.0075 - mae: 0.0620 - val_loss: 0.0072 - val_mae: 0.0602\n","Epoch 19/30\n","1349/1349 [==============================] - 39s 29ms/step - loss: 0.0074 - mae: 0.0616 - val_loss: 0.0072 - val_mae: 0.0600\n","Epoch 20/30\n","1349/1349 [==============================] - 39s 29ms/step - loss: 0.0074 - mae: 0.0613 - val_loss: 0.0071 - val_mae: 0.0598\n","Epoch 21/30\n","1349/1349 [==============================] - 39s 29ms/step - loss: 0.0073 - mae: 0.0610 - val_loss: 0.0071 - val_mae: 0.0596\n","Epoch 22/30\n","1349/1349 [==============================] - 30s 22ms/step - loss: 0.0073 - mae: 0.0607 - val_loss: 0.0070 - val_mae: 0.0594\n","Epoch 23/30\n","1349/1349 [==============================] - 31s 23ms/step - loss: 0.0072 - mae: 0.0604 - val_loss: 0.0070 - val_mae: 0.0592\n","Epoch 24/30\n","1349/1349 [==============================] - 30s 22ms/step - loss: 0.0072 - mae: 0.0602 - val_loss: 0.0070 - val_mae: 0.0590\n","Epoch 25/30\n","1349/1349 [==============================] - 39s 29ms/step - loss: 0.0072 - mae: 0.0600 - val_loss: 0.0069 - val_mae: 0.0589\n","Epoch 26/30\n","1349/1349 [==============================] - 30s 23ms/step - loss: 0.0071 - mae: 0.0598 - val_loss: 0.0069 - val_mae: 0.0587\n","Epoch 27/30\n","1349/1349 [==============================] - 39s 29ms/step - loss: 0.0071 - mae: 0.0596 - val_loss: 0.0069 - val_mae: 0.0586\n","Epoch 28/30\n","1349/1349 [==============================] - 39s 29ms/step - loss: 0.0071 - mae: 0.0594 - val_loss: 0.0069 - val_mae: 0.0585\n","Epoch 29/30\n","1349/1349 [==============================] - 30s 22ms/step - loss: 0.0071 - mae: 0.0592 - val_loss: 0.0069 - val_mae: 0.0584\n","Epoch 30/30\n","1349/1349 [==============================] - 31s 23ms/step - loss: 0.0070 - mae: 0.0590 - val_loss: 0.0068 - val_mae: 0.0583\n","2699/2699 [==============================] - 22s 8ms/step\n","MAE alcanzado en forecast: 0.048282568768495786\n"]}],"source":["# This function loads the data from CSV file, normalizes the data and\n","# splits the dataset into train and validation data. It also uses\n","# windowed_dataset() to split the data into windows of observations and\n","# targets. Finally it defines, compiles and trains a neural network. This\n","# function returns the final trained model.\n","\n","# COMPLETE THE CODE IN THIS FUNCTION\n","def solution_model():\n","    # Downloads and extracts the dataset to the directory that\n","    # contains this file.\n","    download_and_extract_data()\n","    # Reads the dataset from the CSV.\n","    df = pd.read_csv('household_power_consumption.csv', sep=',',\n","                     infer_datetime_format=True, index_col='datetime', header=0)\n","\n","    # Number of features in the dataset. We use all features as predictors to\n","    # predict all features at future time steps.\n","    N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n","\n","    # Normalizes the data\n","    data = df.values\n","    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n","\n","    # Splits the data into training and validation sets.\n","    SPLIT_TIME = int(len(data) * 0.5) # DO NOT CHANGE THIS\n","    print(int(len(data)))\n","    time = np.arange(int(len(data)))\n","    time_train = time[:SPLIT_TIME]\n","    x_train = data[:SPLIT_TIME]\n","    time_valid = time[SPLIT_TIME:]\n","    x_valid = data[SPLIT_TIME:]\n","\n","    # DO NOT CHANGE THIS CODE\n","    tf.keras.backend.clear_session()\n","    tf.random.set_seed(42)\n","\n","    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n","    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n","    # In other cases, it is advised not to change the batch size since it\n","    # might affect your final scores. While setting it to a lower size\n","    # might not do any harm, higher sizes might affect your scores.\n","    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n","\n","    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n","    # on the server.\n","    # Number of past time steps based on which future observations should be\n","    # predicted\n","    N_PAST = 24  # DO NOT CHANGE THIS\n","\n","    # Number of future time steps which are to be predicted.\n","    N_FUTURE = 24  # DO NOT CHANGE THIS\n","\n","    # By how many positions the window slides to create a new window\n","    # of observations.\n","    SHIFT = 1  # DO NOT CHANGE THIS\n","\n","    # Code to create windowed train and validation datasets.\n","    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n","                                 n_past=N_PAST, n_future=N_FUTURE,\n","                                 shift=SHIFT)\n","    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n","                                 n_past=N_PAST, n_future=N_FUTURE,\n","                                 shift=SHIFT)\n","    \n","    # VALIDATE INPUT AND OUTPUT SHAPE\n","    for example_inputs, example_labels in train_set.take(1):\n","        print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n","        print(f'Labels shape (batch, time, features): {example_labels.shape}')\n","\n","    # Find an appropiate learning rate\n","    model = tf.keras.models.Sequential([      \n","      tf.keras.layers.Dense(BATCH_SIZE, input_shape=[N_PAST, N_FEATURES]), # (BATCH_SIZE, N_PAST = 24, N_FEATURES = 7)                          \n","      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, input_shape=[None, N_FEATURES])),# input_shape=train_set_shape),\n","      tf.keras.layers.Dense(30, activation=\"relu\"),\n","      tf.keras.layers.Dense(10, activation=\"relu\"),\n","      tf.keras.layers.Dense(N_FEATURES)\n","    ])\n","\n","    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n","        lambda epoch: 1e-3 * 10 ** (epoch / 20)) # 0.01\n","    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n","    model.compile(loss=tf.keras.losses.Huber(),\n","                  optimizer=optimizer,\n","                  metrics=[\"mae\"])\n","    history = model.fit(train_set, epochs=20, callbacks=[lr_schedule])\n","\n","    plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n","    plt.axis([1e-3, 1e-1, 0, 0.05])\n","    plt.show()\n","    \n","\n","    # Early Stopping Callbacks\n","    early_stopping = tf.keras.callbacks.EarlyStopping(patience=10) # Detener el entrenamiento si el modelo no mejora\n","    model_checkpoint = tf.keras.callbacks.ModelCheckpoint( # De todas las iteraciones, guardamos el mejor modelo\n","        \"c5q4.h5\", save_best_only=True\n","    )\n","\n","    np.random.seed(42)\n","\n","    # Code to define your model.\n","    model = tf.keras.models.Sequential([\n","\n","        # ADD YOUR LAYERS HERE.\n","\n","        # If you don't follow the instructions in the following comments,\n","        # tests will fail to grade your code:\n","        # The input layer of your model must have an input shape of:\n","        # (BATCH_SIZE, N_PAST = 24, N_FEATURES = 7)\n","        # The model must have an output shape of:\n","        # (BATCH_SIZE, N_FUTURE = 24, N_FEATURES = 7).\n","        # Make sure that there are N_FEATURES = 7 neurons in the final dense\n","        # layer since the model predicts 7 features.\n","\n","        # HINT: Bidirectional LSTMs may help boost your score. This is only a\n","        # suggestion.\n","\n","        # WARNING: If you are using the GRU layer, it is advised not to use the\n","        # recurrent_dropout argument (you can alternatively set it to 0),\n","        # since it has not been implemented in the cuDNN kernel and may\n","        # result in much longer training times.\n","\n","        #### BASELINE ####\n","        #tf.keras.layers.Dense(10, activation='relu'),\n","        #tf.keras.layers.Dense(30, activation=\"relu\"),\n","        #tf.keras.layers.Dense(10, activation=\"relu\"),\n","\n","        #### CNN ####\n","        #tf.keras.layers.Conv1D(filters=32,\n","        #                       kernel_size=(N_PAST,),\n","        #                       activation='relu'),\n","        #tf.keras.layers.Dense(units=32, activation='relu'),\n","\n","        #### LSTM ####\n","        tf.keras.layers.Dense(BATCH_SIZE, input_shape=[N_PAST, N_FEATURES]),\n","        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, input_shape=[None, N_FEATURES])),# input_shape=train_set_shape),\n","        tf.keras.layers.Dense(30, activation=\"relu\"),\n","        tf.keras.layers.Dense(10, activation=\"relu\"),\n","        tf.keras.layers.Dense(N_FEATURES)\n","    ])\n","\n","    # Code to train and compile the model\n","    # Cambiar el learning, bajarlo a 1e-6\n","    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, # learning_rate=1e-6,\n","                                        momentum=0.9)\n","    model.compile(\n","        loss=tf.keras.losses.Huber(), # Sugerencias: Poner MAE como loss function, Huber para muchos outliers\n","        optimizer=optimizer,\n","        metrics=[\"mae\"]\n","    )\n","\n","    model.fit(\n","        train_set, epochs=30,\n","        validation_data=valid_set,\n","        callbacks=[early_stopping, model_checkpoint]\n","    )\n","\n","    # PASS THE NORMALIZED data IN THE FOLLOWING CODE\n","    rnn_forecast = model_forecast(model, data, N_PAST, BATCH_SIZE)\n","    rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0, :] # predicciones que corresponden al set de validacion  \n","\n","    x_valid = x_valid[:rnn_forecast.shape[0]]\n","    result = mae(x_valid, rnn_forecast)\n","    # plot_series(np.arange(0, SPLIT_TIME), rnn_forecast)\n","    print(f'MAE alcanzado en forecast: {result}')\n","\n","    return model\n","\n","\n","# Note that you'll need to save your model as a .h5 like this.\n","# When you press the Submit and Test button, your saved .h5 model will\n","# be sent to the testing infrastructure for scoring\n","# and the score will be returned to you.\n","\n","if __name__ == '__main__':\n","    model = solution_model()\n","    # model.save(\"c5q4.h5\") # Comentándola evitamos sobreescribir al mejor modelo de ModelCheckpoint\n"]},{"cell_type":"markdown","metadata":{"id":"YgB0aX9dWCmU"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDngspVXUWS8","outputId":"e9fb356c-497d-4dac-cc08-2332bddc9c56"},"outputs":[{"name":"stdout","output_type":"stream","text":["86400\n","Inputs shape (batch, time, features): (32, 24, 7)\n","Labels shape (batch, time, features): (32, 24, 7)\n","Epoch 1/20\n"]}],"source":["# This function loads the data from CSV file, normalizes the data and\n","# splits the dataset into train and validation data. It also uses\n","# windowed_dataset() to split the data into windows of observations and\n","# targets. Finally it defines, compiles and trains a neural network. This\n","# function returns the final trained model.\n","\n","# COMPLETE THE CODE IN THIS FUNCTION\n","def solution_model():\n","    # Downloads and extracts the dataset to the directory that\n","    # contains this file.\n","    download_and_extract_data()\n","    # Reads the dataset from the CSV.\n","    df = pd.read_csv('household_power_consumption.csv', sep=',',\n","                     infer_datetime_format=True, index_col='datetime', header=0)\n","\n","    # Number of features in the dataset. We use all features as predictors to\n","    # predict all features at future time steps.\n","    N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n","\n","    # Normalizes the data\n","    data = df.values\n","    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n","\n","    # Splits the data into training and validation sets.\n","    SPLIT_TIME = int(len(data) * 0.5) # DO NOT CHANGE THIS\n","    print(int(len(data)))\n","    time = np.arange(int(len(data)))\n","    time_train = time[:SPLIT_TIME]\n","    x_train = data[:SPLIT_TIME]\n","    time_valid = time[SPLIT_TIME:]\n","    x_valid = data[SPLIT_TIME:]\n","\n","    # DO NOT CHANGE THIS CODE\n","    tf.keras.backend.clear_session()\n","    tf.random.set_seed(42)\n","\n","    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n","    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n","    # In other cases, it is advised not to change the batch size since it\n","    # might affect your final scores. While setting it to a lower size\n","    # might not do any harm, higher sizes might affect your scores.\n","    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n","\n","    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n","    # on the server.\n","    # Number of past time steps based on which future observations should be\n","    # predicted\n","    N_PAST = 24  # DO NOT CHANGE THIS\n","\n","    # Number of future time steps which are to be predicted.\n","    N_FUTURE = 24  # DO NOT CHANGE THIS\n","\n","    # By how many positions the window slides to create a new window\n","    # of observations.\n","    SHIFT = 1  # DO NOT CHANGE THIS\n","\n","    # Code to create windowed train and validation datasets.\n","    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n","                                 n_past=N_PAST, n_future=N_FUTURE,\n","                                 shift=SHIFT)\n","    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n","                                 n_past=N_PAST, n_future=N_FUTURE,\n","                                 shift=SHIFT)\n","    \n","    # VALIDATE INPUT AND OUTPUT SHAPE\n","    for example_inputs, example_labels in train_set.take(1):\n","        print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n","        print(f'Labels shape (batch, time, features): {example_labels.shape}')\n","\n","    # Find an appropiate learning rate\n","    model = tf.keras.models.Sequential([      \n","      tf.keras.layers.Dense(BATCH_SIZE, input_shape=[N_PAST, N_FEATURES]), # (BATCH_SIZE, N_PAST = 24, N_FEATURES = 7)                          \n","      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, input_shape=[None, N_FEATURES])),# input_shape=train_set_shape),\n","      tf.keras.layers.Dense(30, activation=\"relu\"),\n","      tf.keras.layers.Dense(10, activation=\"relu\"),\n","      tf.keras.layers.Dense(N_FEATURES)\n","    ])\n","\n","    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n","        lambda epoch: 1e-3 * 10 ** (epoch / 20)) # 0.01\n","    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n","    model.compile(loss=tf.keras.losses.Huber(),\n","                  optimizer=optimizer,\n","                  metrics=[\"mae\"])\n","    history = model.fit(train_set, epochs=20, callbacks=[lr_schedule])\n","\n","    plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n","    plt.axis([1e-3, 1e-1, 0, 0.05])\n","    plt.show()\n","    \n","\n","    # Early Stopping Callbacks\n","    early_stopping = tf.keras.callbacks.EarlyStopping(patience=10) # Detener el entrenamiento si el modelo no mejora\n","    model_checkpoint = tf.keras.callbacks.ModelCheckpoint( # De todas las iteraciones, guardamos el mejor modelo\n","        \"c5q4.h5\", save_best_only=True\n","    )\n","\n","    np.random.seed(42)\n","\n","    # Code to define your model.\n","    model = tf.keras.models.Sequential([\n","\n","        # ADD YOUR LAYERS HERE.\n","\n","        # If you don't follow the instructions in the following comments,\n","        # tests will fail to grade your code:\n","        # The input layer of your model must have an input shape of:\n","        # (BATCH_SIZE, N_PAST = 24, N_FEATURES = 7)\n","        # The model must have an output shape of:\n","        # (BATCH_SIZE, N_FUTURE = 24, N_FEATURES = 7).\n","        # Make sure that there are N_FEATURES = 7 neurons in the final dense\n","        # layer since the model predicts 7 features.\n","\n","        # HINT: Bidirectional LSTMs may help boost your score. This is only a\n","        # suggestion.\n","\n","        # WARNING: If you are using the GRU layer, it is advised not to use the\n","        # recurrent_dropout argument (you can alternatively set it to 0),\n","        # since it has not been implemented in the cuDNN kernel and may\n","        # result in much longer training times.\n","\n","        #### BASELINE ####\n","        #tf.keras.layers.Dense(10, activation='relu'),\n","        #tf.keras.layers.Dense(30, activation=\"relu\"),\n","        #tf.keras.layers.Dense(10, activation=\"relu\"),\n","\n","        #### CNN ####\n","        tf.keras.layers.Conv1D(filters=32,\n","                              kernel_size=(N_PAST,),\n","                               activation='relu'),\n","        tf.keras.layers.Dense(units=32, activation='relu'),\n","\n","\n","    ])\n","\n","    # Code to train and compile the model\n","    # Cambiar el learning, bajarlo a 1e-6\n","    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-6, # learning_rate=1e-6,\n","                                        momentum=0.9)\n","    model.compile(\n","        loss=tf.keras.losses.Huber(), # Sugerencias: Poner MAE como loss function, Huber para muchos outliers\n","        optimizer=optimizer,\n","        metrics=[\"mae\"]\n","    )\n","\n","    model.fit(\n","        train_set, epochs=30,\n","        validation_data=valid_set,\n","        callbacks=[early_stopping, model_checkpoint]\n","    )\n","\n","    # PASS THE NORMALIZED data IN THE FOLLOWING CODE\n","    rnn_forecast = model_forecast(model, data, N_PAST, BATCH_SIZE)\n","    rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0, :] # predicciones que corresponden al set de validacion  \n","\n","    x_valid = x_valid[:rnn_forecast.shape[0]]\n","    result = mae(x_valid, rnn_forecast)\n","    # plot_series(np.arange(0, SPLIT_TIME), rnn_forecast)\n","    print(f'MAE alcanzado en forecast: {result}')\n","\n","    return model\n","\n","\n","# Note that you'll need to save your model as a .h5 like this.\n","# When you press the Submit and Test button, your saved .h5 model will\n","# be sent to the testing infrastructure for scoring\n","# and the score will be returned to you.\n","\n","if __name__ == '__main__':\n","    model = solution_model()\n","    # model.save(\"c5q4.h5\") # Comentándola evitamos sobreescribir al mejor modelo de ModelCheckpoint\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZgdgyQFbVrN"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}